{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bebcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: MODEL COMPARISON (Qwen 3B vs 7B)\n",
      "================================================================================\n",
      "\n",
      "Initializing ChromaDB from ../../Chroma_db_database...\n",
      "âœ“ Collection 'policy_docs' loaded successfully\n",
      "âœ“ Total documents in collection: 34\n",
      "\n",
      "================================================================================\n",
      "STARTING MODEL COMPARISON EXPERIMENT\n",
      "================================================================================\n",
      "Total tests to run: 26\n",
      "Questions: 13\n",
      "Models: ['qwen2.5:3b-instruct', 'qwen2.5:7b']\n",
      "Top-K: 3\n",
      "\n",
      "Model Information:\n",
      "--------------------------------------------------------------------------------\n",
      "  qwen2.5:3b-instruct: 3.1B parameters\n",
      "  qwen2.5:7b: 7.6B parameters\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing Model: qwen2.5:3b-instruct\n",
      "================================================================================\n",
      "\n",
      "[1/26] Q1 (List-type, Medium)\n",
      "  Q: What are the three main objectives of the SEDGs guidelines?...\n",
      "  â±  Latency: 14.12s | Tokens: 4189 | Speed: 47.8 tok/s\n",
      "  ðŸ“Š Quality: 0.235 | Coverage: 0.250 | Words: 77\n",
      "\n",
      "[2/26] Q2 (Complex-list, High)\n",
      "  Q: Who are included in Socio-Economically Disadvantaged Groups according ...\n",
      "  â±  Latency: 5.83s | Tokens: 4232 | Speed: 43.6 tok/s\n",
      "  ðŸ“Š Quality: 0.110 | Coverage: 0.000 | Words: 106\n",
      "\n",
      "[3/26] Q3 (Factual, Medium)\n",
      "  Q: What is the composition of the SEDGs Cell governance structure?...\n",
      "  â±  Latency: 5.65s | Tokens: 4218 | Speed: 42.2 tok/s\n",
      "  ðŸ“Š Quality: 0.900 | Coverage: 1.000 | Words: 81\n",
      "\n",
      "[4/26] Q5 (Factual, Low)\n",
      "  Q: How many Ph.D. Excellence Citations does UGC award annually?...\n",
      "  â±  Latency: 4.41s | Tokens: 4155 | Speed: 38.8 tok/s\n",
      "  ðŸ“Š Quality: 0.200 | Coverage: 0.200 | Words: 41\n",
      "\n",
      "[5/26] Q7 (Complex-list, High)\n",
      "  Q: What are the key criteria for shortlisting candidates for PhD Excellen...\n",
      "  â±  Latency: 5.31s | Tokens: 4201 | Speed: 43.2 tok/s\n",
      "  ðŸ“Š Quality: 0.827 | Coverage: 0.833 | Words: 84\n",
      "\n",
      "[6/26] Q9 (List-type, Medium)\n",
      "  Q: What are the three main objectives of NEP SAARTHI initiative?...\n",
      "  â±  Latency: 7.28s | Tokens: 4210 | Speed: 44.4 tok/s\n",
      "  ðŸ“Š Quality: 0.110 | Coverage: 0.000 | Words: 92\n",
      "\n",
      "[7/26] Q11 (Complex-list, High)\n",
      "  Q: What are the roles and responsibilities of NEP SAARTHI?...\n",
      "  â±  Latency: 8.11s | Tokens: 4233 | Speed: 44.7 tok/s\n",
      "  ðŸ“Š Quality: 0.100 | Coverage: 0.000 | Words: 106\n",
      "\n",
      "[8/26] Q13 (List-type, Medium)\n",
      "  Q: What are the four main objectives of University-Industry linkage guide...\n",
      "  â±  Latency: 5.28s | Tokens: 4203 | Speed: 43.9 tok/s\n",
      "  ðŸ“Š Quality: 0.110 | Coverage: 0.000 | Words: 86\n",
      "\n",
      "[9/26] Q16 (Factual, Low)\n",
      "  Q: What are the two categories of undergraduate internships?...\n",
      "  â±  Latency: 5.67s | Tokens: 4229 | Speed: 44.6 tok/s\n",
      "  ðŸ“Š Quality: 0.367 | Coverage: 0.333 | Words: 107\n",
      "\n",
      "[10/26] Q17 (Factual, Low)\n",
      "  Q: How many credits are assigned for internship in UG degree programmes?...\n",
      "  â±  Latency: 5.00s | Tokens: 4193 | Speed: 43.3 tok/s\n",
      "  ðŸ“Š Quality: 0.100 | Coverage: 0.000 | Words: 77\n",
      "\n",
      "[11/26] Q18 (Complex-list, High)\n",
      "  Q: What are the 10 main objectives of engaging undergraduates in internsh...\n",
      "  â±  Latency: 9.88s | Tokens: 4432 | Speed: 46.9 tok/s\n",
      "  ðŸ“Š Quality: 0.153 | Coverage: 0.125 | Words: 239\n",
      "\n",
      "[12/26] Q_OUT1 (Out-of-scope, N/A)\n",
      "  Q: What is the capital of France?...\n",
      "  â±  Latency: 6.57s | Tokens: 4104 | Speed: 19.7 tok/s\n",
      "  ðŸ“Š Quality: 0.000 | Coverage: 0.000 | Words: 6\n",
      "\n",
      "[13/26] Q_OUT2 (Out-of-scope, N/A)\n",
      "  Q: How do I apply for a student loan?...\n",
      "  â±  Latency: 11.96s | Tokens: 4427 | Speed: 46.4 tok/s\n",
      "  ðŸ“Š Quality: 0.000 | Coverage: 0.000 | Words: 277\n",
      "\n",
      "================================================================================\n",
      "Testing Model: qwen2.5:7b\n",
      "================================================================================\n",
      "\n",
      "[14/26] Q1 (List-type, Medium)\n",
      "  Q: What are the three main objectives of the SEDGs guidelines?...\n",
      "  â±  Latency: 46.45s | Tokens: 4206 | Speed: 4.4 tok/s\n",
      "  ðŸ“Š Quality: 0.235 | Coverage: 0.250 | Words: 81\n",
      "\n",
      "[15/26] Q2 (Complex-list, High)\n",
      "  Q: Who are included in Socio-Economically Disadvantaged Groups according ...\n",
      "  â±  Latency: 44.08s | Tokens: 4198 | Speed: 4.5 tok/s\n",
      "  ðŸ“Š Quality: 0.181 | Coverage: 0.143 | Words: 56\n",
      "\n",
      "[16/26] Q3 (Factual, Medium)\n",
      "  Q: What is the composition of the SEDGs Cell governance structure?...\n",
      "  â±  Latency: 52.04s | Tokens: 4218 | Speed: 4.5 tok/s\n",
      "  ðŸ“Š Quality: 0.900 | Coverage: 1.000 | Words: 78\n",
      "\n",
      "[17/26] Q5 (Factual, Low)\n",
      "  Q: How many Ph.D. Excellence Citations does UGC award annually?...\n",
      "  â±  Latency: 35.35s | Tokens: 4167 | Speed: 4.7 tok/s\n",
      "  ðŸ“Š Quality: 0.400 | Coverage: 0.400 | Words: 59\n",
      "\n",
      "[18/26] Q7 (Complex-list, High)\n",
      "  Q: What are the key criteria for shortlisting candidates for PhD Excellen...\n",
      "  â±  Latency: 34.57s | Tokens: 4161 | Speed: 4.6 tok/s\n",
      "  ðŸ“Š Quality: 0.110 | Coverage: 0.000 | Words: 57\n",
      "\n",
      "[19/26] Q9 (List-type, Medium)\n",
      "  Q: What are the three main objectives of NEP SAARTHI initiative?...\n",
      "  â±  Latency: 44.91s | Tokens: 4190 | Speed: 4.4 tok/s\n",
      "  ðŸ“Š Quality: 0.110 | Coverage: 0.000 | Words: 67\n",
      "\n",
      "[20/26] Q11 (Complex-list, High)\n",
      "  Q: What are the roles and responsibilities of NEP SAARTHI?...\n",
      "  â±  Latency: 52.47s | Tokens: 4221 | Speed: 4.5 tok/s\n",
      "  ðŸ“Š Quality: 0.110 | Coverage: 0.000 | Words: 94\n",
      "\n",
      "[21/26] Q13 (List-type, Medium)\n",
      "  Q: What are the four main objectives of University-Industry linkage guide...\n",
      "  â±  Latency: 46.10s | Tokens: 4222 | Speed: 4.8 tok/s\n",
      "  ðŸ“Š Quality: 0.210 | Coverage: 0.200 | Words: 96\n",
      "\n",
      "[22/26] Q16 (Factual, Low)\n",
      "  Q: What are the two categories of undergraduate internships?...\n",
      "  â±  Latency: 32.87s | Tokens: 4165 | Speed: 5.2 tok/s\n",
      "  ðŸ“Š Quality: 0.367 | Coverage: 0.333 | Words: 57\n",
      "\n",
      "[23/26] Q17 (Factual, Low)\n",
      "  Q: How many credits are assigned for internship in UG degree programmes?...\n",
      "  â±  Latency: 29.03s | Tokens: 4146 | Speed: 5.3 tok/s\n",
      "  ðŸ“Š Quality: 0.100 | Coverage: 0.000 | Words: 35\n",
      "\n",
      "[24/26] Q18 (Complex-list, High)\n",
      "  Q: What are the 10 main objectives of engaging undergraduates in internsh...\n",
      "  â±  Latency: 62.91s | Tokens: 4300 | Speed: 4.7 tok/s\n",
      "  ðŸ“Š Quality: 0.173 | Coverage: 0.125 | Words: 164\n",
      "\n",
      "[25/26] Q_OUT1 (Out-of-scope, N/A)\n",
      "  Q: What is the capital of France?...\n",
      "  â±  Latency: 24.61s | Tokens: 4104 | Speed: 5.0 tok/s\n",
      "  ðŸ“Š Quality: 0.000 | Coverage: 0.000 | Words: 6\n",
      "\n",
      "[26/26] Q_OUT2 (Out-of-scope, N/A)\n",
      "  Q: How do I apply for a student loan?...\n",
      "  â±  Latency: 95.27s | Tokens: 4412 | Speed: 4.1 tok/s\n",
      "  ðŸ“Š Quality: 0.000 | Coverage: 0.000 | Words: 235\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETED\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS: MODEL COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "1. OVERALL PERFORMANCE (In-Scope Questions):\n",
      "--------------------------------------------------------------------------------\n",
      "                    latency_sec                       output_tokens_per_sec  \\\n",
      "                           mean     std    min    max                  mean   \n",
      "model                                                                         \n",
      "qwen2.5:3b-instruct       6.958   2.859   4.41  14.12                43.931   \n",
      "qwen2.5:7b               43.707  10.080  29.03  62.91                 4.687   \n",
      "\n",
      "                           quality_score        key_term_coverage         \\\n",
      "                       std          mean    std              mean    std   \n",
      "model                                                                      \n",
      "qwen2.5:3b-instruct  2.356         0.292  0.294             0.249  0.352   \n",
      "qwen2.5:7b           0.300         0.263  0.235             0.223  0.294   \n",
      "\n",
      "                    word_count total_tokens cpu_usage_percent memory_mb  \n",
      "                          mean         mean              mean      mean  \n",
      "model                                                                    \n",
      "qwen2.5:3b-instruct     99.636     4226.818            23.082   178.963  \n",
      "qwen2.5:7b              76.727     4199.455            15.905   160.075  \n",
      "\n",
      "2. HEAD-TO-HEAD COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Metric                         3B Model        7B Model        Winner    \n",
      "----------------------------------------------------------------------\n",
      "Avg Latency (s)                6.958           43.707          3B âœ“      \n",
      "Avg Quality Score              0.292           0.263           3B âœ“      \n",
      "Avg Coverage                   0.249           0.223           3B âœ“      \n",
      "Tokens/sec                     43.931          4.687           3B âœ“      \n",
      "Word Count                     99.636          76.727          -         \n",
      "CPU Usage (%)                  23.082          15.905          7B âœ“      \n",
      "Memory (MB)                    178.963         160.075         7B âœ“      \n",
      "\n",
      "3. PERFORMANCE BY COMPLEXITY:\n",
      "--------------------------------------------------------------------------------\n",
      "                                quality_score  latency_sec\n",
      "model               complexity                            \n",
      "qwen2.5:3b-instruct High                0.298        7.283\n",
      "                    Low                 0.222        5.027\n",
      "                    Medium              0.339        8.082\n",
      "qwen2.5:7b          High                0.144       48.508\n",
      "                    Low                 0.289       32.417\n",
      "                    Medium              0.364       47.375\n",
      "\n",
      "4. PERFORMANCE BY QUESTION CATEGORY:\n",
      "--------------------------------------------------------------------------------\n",
      "                                  quality_score  key_term_coverage\n",
      "model               category                                      \n",
      "qwen2.5:3b-instruct Complex-list          0.298              0.240\n",
      "                    Factual               0.392              0.383\n",
      "                    List-type             0.152              0.083\n",
      "qwen2.5:7b          Complex-list          0.144              0.067\n",
      "                    Factual               0.442              0.433\n",
      "                    List-type             0.185              0.150\n",
      "\n",
      "5. HALLUCINATION TEST (Out-of-Scope Questions):\n",
      "--------------------------------------------------------------------------------\n",
      "                    hallucination_score      \n",
      "                                   mean count\n",
      "model                                        \n",
      "qwen2.5:3b-instruct                 0.0     2\n",
      "qwen2.5:7b                          0.0     2\n",
      "\n",
      "Note: Score of 1.0 means model correctly refused to answer\n",
      "\n",
      "âœ“ Detailed results saved to: ./experiment_results/model_comparison_20251206_165449.csv\n",
      "âœ“ Summary saved to: ./experiment_results/model_summary_20251206_165449.csv\n",
      "âœ“ Analysis saved to: ./experiment_results/model_analysis_20251206_165449.json\n",
      "\n",
      "âœ“ Plots saved to: ./experiment_results/model_comparison_plots_20251206_165450.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Sample Results:\n",
      "                 model question_id  latency_sec  quality_score  \\\n",
      "0  qwen2.5:3b-instruct          Q1        14.12          0.235   \n",
      "1  qwen2.5:3b-instruct          Q2         5.83          0.110   \n",
      "2  qwen2.5:3b-instruct          Q3         5.65          0.900   \n",
      "3  qwen2.5:3b-instruct          Q5         4.41          0.200   \n",
      "4  qwen2.5:3b-instruct          Q7         5.31          0.827   \n",
      "5  qwen2.5:3b-instruct          Q9         7.28          0.110   \n",
      "6  qwen2.5:3b-instruct         Q11         8.11          0.100   \n",
      "7  qwen2.5:3b-instruct         Q13         5.28          0.110   \n",
      "8  qwen2.5:3b-instruct         Q16         5.67          0.367   \n",
      "9  qwen2.5:3b-instruct         Q17         5.00          0.100   \n",
      "\n",
      "   output_tokens_per_sec  \n",
      "0                  47.79  \n",
      "1                  43.60  \n",
      "2                  42.20  \n",
      "3                  38.76  \n",
      "4                  43.21  \n",
      "5                  44.39  \n",
      "6                  44.73  \n",
      "7                  43.85  \n",
      "8                  44.60  \n",
      "9                  43.26  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Experiment 1: Model Comparison (Qwen 3B vs 7B)\n",
    "Purpose: Compare latency, answer quality, and resource usage between models\n",
    "Author: RAG Policy-as-Code Team\n",
    "Date: December 2025\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import ollama\n",
    "import json\n",
    "import psutil\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CHROMA_DB_PATH = \"../../Chroma_db_database\"\n",
    "COLLECTION_NAME = \"policy_docs\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "MODELS_TO_TEST = [\"qwen2.5:3b-instruct\", \"qwen2.5:7b\"]\n",
    "TOP_K = 3  # Use optimal K from previous experiment\n",
    "\n",
    "# Gold Standard Dataset (expanded for model comparison)\n",
    "gold_standard = {\n",
    "    \"Q1\": {\n",
    "        \"question\": \"What are the three main objectives of the SEDGs guidelines?\",\n",
    "        \"gold_answer\": \"improving equitable access, extending basic facilities, setting up SEDGs Cell\",\n",
    "        \"key_terms\": [\"equitable access\", \"facilities\", \"SEDGs Cell\", \"objectives\"],\n",
    "        \"category\": \"List-type\",\n",
    "        \"complexity\": \"Medium\"\n",
    "    },\n",
    "    \"Q2\": {\n",
    "        \"question\": \"Who are included in Socio-Economically Disadvantaged Groups according to NEP 2020?\",\n",
    "        \"gold_answer\": \"gender identity, social backwardness, educational backward, minority, disabilities, vulnerable groups, disadvantaged locations\",\n",
    "        \"key_terms\": [\"SCs\", \"STs\", \"OBCs\", \"transgender\", \"minority\", \"disabilities\", \"EWS\"],\n",
    "        \"category\": \"Complex-list\",\n",
    "        \"complexity\": \"High\"\n",
    "    },\n",
    "    \"Q3\": {\n",
    "        \"question\": \"What is the composition of the SEDGs Cell governance structure?\",\n",
    "        \"gold_answer\": \"Chairperson, Senior Professor, In-charge of Internal Complaint Committee, Coordinator/Director of IQAC, SC/ST Representative, OBC Representative, Two Students Representatives, Assistant Registrar\",\n",
    "        \"key_terms\": [\"Chairperson\", \"Professor\", \"IQAC\", \"SC/ST Representative\", \"OBC Representative\", \"Students\", \"Assistant Registrar\"],\n",
    "        \"category\": \"Factual\",\n",
    "        \"complexity\": \"Medium\"\n",
    "    },\n",
    "    \"Q5\": {\n",
    "        \"question\": \"How many Ph.D. Excellence Citations does UGC award annually?\",\n",
    "        \"gold_answer\": \"ten PhD Excellence Citations, two from each discipline\",\n",
    "        \"key_terms\": [\"ten\", \"two\", \"citations\", \"disciplines\", \"annually\"],\n",
    "        \"category\": \"Factual\",\n",
    "        \"complexity\": \"Low\"\n",
    "    },\n",
    "    \"Q7\": {\n",
    "        \"question\": \"What are the key criteria for shortlisting candidates for PhD Excellence Citation?\",\n",
    "        \"gold_answer\": \"Originality, Innovation, Contribution to knowledge, Methodology, Clarity, Impact, References, Presentation, Defense\",\n",
    "        \"key_terms\": [\"Originality\", \"Innovation\", \"Contribution\", \"Methodology\", \"Impact\", \"References\"],\n",
    "        \"category\": \"Complex-list\",\n",
    "        \"complexity\": \"High\"\n",
    "    },\n",
    "    \"Q9\": {\n",
    "        \"question\": \"What are the three main objectives of NEP SAARTHI initiative?\",\n",
    "        \"gold_answer\": \"create awareness, encourage participation, establish feedback mechanism\",\n",
    "        \"key_terms\": [\"awareness\", \"participation\", \"feedback\", \"NEP 2020\"],\n",
    "        \"category\": \"List-type\",\n",
    "        \"complexity\": \"Medium\"\n",
    "    },\n",
    "    \"Q11\": {\n",
    "        \"question\": \"What are the roles and responsibilities of NEP SAARTHI?\",\n",
    "        \"gold_answer\": \"Work as ambassador, conduct awareness drives, organize activities, disseminate information, establish dialogue, prepare notes, attend programmes, collect feedback\",\n",
    "        \"key_terms\": [\"ambassador\", \"awareness\", \"activities\", \"disseminate\", \"dialogue\", \"feedback\"],\n",
    "        \"category\": \"Complex-list\",\n",
    "        \"complexity\": \"High\"\n",
    "    },\n",
    "    \"Q13\": {\n",
    "        \"question\": \"What are the four main objectives of University-Industry linkage guidelines?\",\n",
    "        \"gold_answer\": \"promote R&D, develop skill sets, establish linkages, create training opportunities\",\n",
    "        \"key_terms\": [\"R&D\", \"skill sets\", \"linkages\", \"training\", \"apprenticeship\"],\n",
    "        \"category\": \"List-type\",\n",
    "        \"complexity\": \"Medium\"\n",
    "    },\n",
    "    \"Q16\": {\n",
    "        \"question\": \"What are the two categories of undergraduate internships?\",\n",
    "        \"gold_answer\": \"enhancing employability and developing research aptitude\",\n",
    "        \"key_terms\": [\"employability\", \"research aptitude\", \"categories\"],\n",
    "        \"category\": \"Factual\",\n",
    "        \"complexity\": \"Low\"\n",
    "    },\n",
    "    \"Q17\": {\n",
    "        \"question\": \"How many credits are assigned for internship in UG degree programmes?\",\n",
    "        \"gold_answer\": \"2-4 credits, 60-120 hours duration, 12 credits for research project\",\n",
    "        \"key_terms\": [\"2-4 credits\", \"60-120 hours\", \"12 credits\", \"mandatory\"],\n",
    "        \"category\": \"Factual\",\n",
    "        \"complexity\": \"Low\"\n",
    "    },\n",
    "    \"Q18\": {\n",
    "        \"question\": \"What are the 10 main objectives of engaging undergraduates in internship programmes?\",\n",
    "        \"gold_answer\": \"Integration of workshop with workplace, Understanding world of work, Hybrid learning, Research aptitude, Emerging technologies, Entrepreneurial capabilities, Decision-making, Social imagery, Collaborative influence, Professional competency\",\n",
    "        \"key_terms\": [\"workshop\", \"workplace\", \"hybrid\", \"research\", \"entrepreneurial\", \"decision-making\", \"collaborative\", \"professional\"],\n",
    "        \"category\": \"Complex-list\",\n",
    "        \"complexity\": \"High\"\n",
    "    },\n",
    "    # Out-of-scope questions for hallucination testing\n",
    "    \"Q_OUT1\": {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"gold_answer\": \"I don't have information about this\",\n",
    "        \"key_terms\": [\"don't have information\", \"not available\", \"cannot answer\"],\n",
    "        \"category\": \"Out-of-scope\",\n",
    "        \"complexity\": \"N/A\"\n",
    "    },\n",
    "    \"Q_OUT2\": {\n",
    "        \"question\": \"How do I apply for a student loan?\",\n",
    "        \"gold_answer\": \"I don't have information about this\",\n",
    "        \"key_terms\": [\"don't have information\", \"not available\", \"cannot answer\"],\n",
    "        \"category\": \"Out-of-scope\",\n",
    "        \"complexity\": \"N/A\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_chromadb(db_path: str, collection_name: str):\n",
    "    \"\"\"Initialize ChromaDB client and get collection\"\"\"\n",
    "    print(f\"Initializing ChromaDB from {db_path}...\")\n",
    "    \n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        doc_count = collection.count()\n",
    "        print(f\"âœ“ Collection '{collection_name}' loaded successfully\")\n",
    "        print(f\"âœ“ Total documents in collection: {doc_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error: Collection '{collection_name}' not found\")\n",
    "        print(f\"âœ— Details: {e}\\n\")\n",
    "        raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "\n",
    "def get_model_info(model: str) -> Dict:\n",
    "    \"\"\"Get model information from Ollama\"\"\"\n",
    "    try:\n",
    "        info = ollama.show(model)\n",
    "        return {\n",
    "            'model': model,\n",
    "            'parameters': info.get('details', {}).get('parameter_size', 'Unknown'),\n",
    "            'quantization': info.get('details', {}).get('quantization_level', 'Unknown'),\n",
    "            'family': info.get('details', {}).get('family', 'Unknown')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not get model info for {model}: {e}\")\n",
    "        return {'model': model, 'parameters': 'Unknown'}\n",
    "\n",
    "\n",
    "def get_system_resources() -> Dict:\n",
    "    \"\"\"Get current system resource usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return {\n",
    "        'cpu_percent': psutil.cpu_percent(interval=0.1),\n",
    "        'memory_mb': process.memory_info().rss / 1024 / 1024,\n",
    "        'memory_percent': process.memory_percent()\n",
    "    }\n",
    "\n",
    "\n",
    "def embed_query(query: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
    "    \"\"\"Generate embedding for a query using Ollama\"\"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=model, prompt=query)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def retrieve_segments(collection, query_embedding: List[float], top_k: int) -> Dict:\n",
    "    \"\"\"Retrieve top-k segments from ChromaDB\"\"\"\n",
    "    try:\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving segments: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_context(retrieved_results: Dict, top_k: int) -> str:\n",
    "    \"\"\"Build context string from retrieved results\"\"\"\n",
    "    documents = retrieved_results['documents'][0][:top_k]\n",
    "    metadatas = retrieved_results['metadatas'][0][:top_k]\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, (doc, meta) in enumerate(zip(documents, metadatas)):\n",
    "        source = meta.get('source', f'Document_{i+1}')\n",
    "        context_parts.append(f\"[Source: {source}]\\n{doc}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def construct_rag_prompt(context: str, question: str) -> str:\n",
    "    \"\"\"Construct prompt for RAG\"\"\"\n",
    "    prompt = f\"\"\"You are an academic compliance expert specializing in UGC guidelines and regulations.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question using ONLY the provided context below\n",
    "2. If the question asks for multiple items (objectives, categories, etc.), list ALL of them\n",
    "3. Be concise, accurate, and factual\n",
    "4. If the context doesn't contain relevant information, clearly state: \"I don't have information about this in the available guidelines\"\n",
    "5. Include source references when possible\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer_with_metrics(prompt: str, model: str) -> Dict:\n",
    "    \"\"\"Generate answer and collect performance metrics\"\"\"\n",
    "    \n",
    "    # Get baseline resources\n",
    "    resources_before = get_system_resources()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                'temperature': 0.3,\n",
    "                'top_p': 0.9,\n",
    "                'num_predict': 512,\n",
    "                'num_ctx': 4096  # Context window\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        resources_after = get_system_resources()\n",
    "        \n",
    "        # Extract tokens info if available\n",
    "        total_duration = response.get('total_duration', 0) / 1e9  # Convert to seconds\n",
    "        prompt_eval_duration = response.get('prompt_eval_duration', 0) / 1e9\n",
    "        eval_duration = response.get('eval_duration', 0) / 1e9\n",
    "        \n",
    "        prompt_eval_count = response.get('prompt_eval_count', 0)\n",
    "        eval_count = response.get('eval_count', 0)\n",
    "        \n",
    "        # Calculate tokens per second\n",
    "        prompt_tokens_per_sec = prompt_eval_count / prompt_eval_duration if prompt_eval_duration > 0 else 0\n",
    "        output_tokens_per_sec = eval_count / eval_duration if eval_duration > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'answer': response['response'],\n",
    "            'latency_sec': round(latency, 2),\n",
    "            'total_duration_sec': round(total_duration, 2),\n",
    "            'prompt_eval_duration_sec': round(prompt_eval_duration, 2),\n",
    "            'eval_duration_sec': round(eval_duration, 2),\n",
    "            'prompt_tokens': prompt_eval_count,\n",
    "            'output_tokens': eval_count,\n",
    "            'total_tokens': prompt_eval_count + eval_count,\n",
    "            'prompt_tokens_per_sec': round(prompt_tokens_per_sec, 2),\n",
    "            'output_tokens_per_sec': round(output_tokens_per_sec, 2),\n",
    "            'cpu_usage_percent': round((resources_after['cpu_percent'] + resources_before['cpu_percent']) / 2, 2),\n",
    "            'memory_mb': round(resources_after['memory_mb'], 2),\n",
    "            'answer_length': len(response['response'])\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer with {model}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_answer_quality(answer: str, gold_data: Dict) -> Dict:\n",
    "    \"\"\"Evaluate answer quality with multiple metrics\"\"\"\n",
    "    \n",
    "    answer_lower = answer.lower()\n",
    "    key_terms = gold_data['key_terms']\n",
    "    gold_answer = gold_data['gold_answer'].lower()\n",
    "    category = gold_data['category']\n",
    "    \n",
    "    # Metric 1: Key term coverage\n",
    "    terms_found = sum(1 for term in key_terms if term.lower() in answer_lower)\n",
    "    key_term_coverage = terms_found / len(key_terms) if key_terms else 0\n",
    "    \n",
    "    # Metric 2: Completeness (for list-type questions)\n",
    "    if category in ['List-type', 'Complex-list']:\n",
    "        # Check if answer contains enumeration\n",
    "        has_enumeration = any(marker in answer for marker in ['1.', '2.', '3.', '-', 'â€¢', '(i)', '(a)'])\n",
    "        completeness_bonus = 0.1 if has_enumeration else 0\n",
    "    else:\n",
    "        completeness_bonus = 0\n",
    "    \n",
    "    # Metric 3: Hallucination detection (for out-of-scope)\n",
    "    if category == 'Out-of-scope':\n",
    "        refusal_terms = [\"don't have information\", \"not available\", \"cannot answer\", \n",
    "                        \"not mentioned\", \"context doesn't contain\", \"no information\"]\n",
    "        has_refusal = any(term in answer_lower for term in refusal_terms)\n",
    "        hallucination_score = 1.0 if has_refusal else 0.0\n",
    "    else:\n",
    "        hallucination_score = None\n",
    "    \n",
    "    # Metric 4: Answer relevance (basic check)\n",
    "    relevance_terms = [term.lower() for term in key_terms[:3]]  # Check first 3 key terms\n",
    "    relevance_score = sum(1 for term in relevance_terms if term in answer_lower) / len(relevance_terms) if relevance_terms else 0\n",
    "    \n",
    "    # Metric 5: Conciseness (penalize overly verbose answers)\n",
    "    word_count = len(answer.split())\n",
    "    conciseness_score = 1.0 if word_count < 200 else 0.8 if word_count < 300 else 0.6\n",
    "    \n",
    "    # Composite quality score\n",
    "    if category == 'Out-of-scope':\n",
    "        quality_score = hallucination_score  # Only check if it refuses correctly\n",
    "    else:\n",
    "        quality_score = (\n",
    "            0.5 * key_term_coverage +\n",
    "            0.3 * relevance_score +\n",
    "            0.1 * conciseness_score +\n",
    "            0.1 * completeness_bonus\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'key_term_coverage': round(key_term_coverage, 3),\n",
    "        'relevance_score': round(relevance_score, 3),\n",
    "        'conciseness_score': round(conciseness_score, 3),\n",
    "        'hallucination_score': hallucination_score,\n",
    "        'quality_score': round(quality_score, 3),\n",
    "        'terms_found': terms_found,\n",
    "        'total_terms': len(key_terms),\n",
    "        'word_count': word_count\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_model_comparison_experiment(\n",
    "    collection,\n",
    "    gold_standard: Dict,\n",
    "    models: List[str],\n",
    "    top_k: int = TOP_K\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run model comparison experiment\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        gold_standard: Dictionary with test questions\n",
    "        models: List of model names to test\n",
    "        top_k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    total_tests = len(gold_standard) * len(models)\n",
    "    current_test = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"STARTING MODEL COMPARISON EXPERIMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total tests to run: {total_tests}\")\n",
    "    print(f\"Questions: {len(gold_standard)}\")\n",
    "    print(f\"Models: {models}\")\n",
    "    print(f\"Top-K: {top_k}\\n\")\n",
    "    \n",
    "    # Get model info\n",
    "    print(\"Model Information:\")\n",
    "    print(\"-\"*80)\n",
    "    for model in models:\n",
    "        info = get_model_info(model)\n",
    "        print(f\"  {model}: {info.get('parameters', 'Unknown')} parameters\")\n",
    "    print()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Testing Model: {model}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for q_id, gold_data in gold_standard.items():\n",
    "            current_test += 1\n",
    "            question = gold_data['question']\n",
    "            category = gold_data['category']\n",
    "            complexity = gold_data.get('complexity', 'Unknown')\n",
    "            \n",
    "            print(f\"\\n[{current_test}/{total_tests}] {q_id} ({category}, {complexity})\")\n",
    "            print(f\"  Q: {question[:70]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Step 1: Embed query (same for both models)\n",
    "                query_embedding = embed_query(question)\n",
    "                if query_embedding is None:\n",
    "                    print(\"  âœ— Failed to generate embedding\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 2: Retrieve segments\n",
    "                retrieved_results = retrieve_segments(collection, query_embedding, top_k)\n",
    "                if retrieved_results is None:\n",
    "                    print(\"  âœ— Failed to retrieve segments\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 3: Build context\n",
    "                context = build_context(retrieved_results, top_k)\n",
    "                context_length = len(context)\n",
    "                \n",
    "                # Step 4: Generate answer with metrics\n",
    "                prompt = construct_rag_prompt(context, question)\n",
    "                prompt_length = len(prompt)\n",
    "                \n",
    "                metrics = generate_answer_with_metrics(prompt, model)\n",
    "                \n",
    "                if metrics is None:\n",
    "                    print(\"  âœ— Failed to generate answer\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 5: Evaluate answer quality\n",
    "                quality_metrics = evaluate_answer_quality(metrics['answer'], gold_data)\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"  â±  Latency: {metrics['latency_sec']:.2f}s | Tokens: {metrics['total_tokens']} | Speed: {metrics['output_tokens_per_sec']:.1f} tok/s\")\n",
    "                print(f\"  ðŸ“Š Quality: {quality_metrics['quality_score']:.3f} | Coverage: {quality_metrics['key_term_coverage']:.3f} | Words: {quality_metrics['word_count']}\")\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'model': model,\n",
    "                    'question_id': q_id,\n",
    "                    'question': question,\n",
    "                    'category': category,\n",
    "                    'complexity': complexity,\n",
    "                    'context_length': context_length,\n",
    "                    'prompt_length': prompt_length,\n",
    "                    **metrics,\n",
    "                    **quality_metrics,\n",
    "                    'answer_preview': metrics['answer'][:200] + \"...\" if len(metrics['answer']) > 200 else metrics['answer']\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT COMPLETED\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def analyze_model_comparison(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze and compare models\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS: MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Separate in-scope and out-of-scope\n",
    "    in_scope_df = results_df[results_df['category'] != 'Out-of-scope']\n",
    "    out_scope_df = results_df[results_df['category'] == 'Out-of-scope']\n",
    "    \n",
    "    print(\"1. OVERALL PERFORMANCE (In-Scope Questions):\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Group by model\n",
    "    summary = in_scope_df.groupby('model').agg({\n",
    "        'latency_sec': ['mean', 'std', 'min', 'max'],\n",
    "        'output_tokens_per_sec': ['mean', 'std'],\n",
    "        'quality_score': ['mean', 'std'],\n",
    "        'key_term_coverage': ['mean', 'std'],\n",
    "        'word_count': 'mean',\n",
    "        'total_tokens': 'mean',\n",
    "        'cpu_usage_percent': 'mean',\n",
    "        'memory_mb': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(summary)\n",
    "    print()\n",
    "    \n",
    "    # Compare models directly\n",
    "    models = results_df['model'].unique()\n",
    "    if len(models) == 2:\n",
    "        model_3b = [m for m in models if '3b' in m.lower()][0]\n",
    "        model_7b = [m for m in models if '7b' in m.lower()][0]\n",
    "        \n",
    "        df_3b = in_scope_df[in_scope_df['model'] == model_3b]\n",
    "        df_7b = in_scope_df[in_scope_df['model'] == model_7b]\n",
    "        \n",
    "        print(\"2. HEAD-TO-HEAD COMPARISON:\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"\\n{'Metric':<30} {'3B Model':<15} {'7B Model':<15} {'Winner':<10}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        metrics_to_compare = [\n",
    "            ('Avg Latency (s)', 'latency_sec', 'lower'),\n",
    "            ('Avg Quality Score', 'quality_score', 'higher'),\n",
    "            ('Avg Coverage', 'key_term_coverage', 'higher'),\n",
    "            ('Tokens/sec', 'output_tokens_per_sec', 'higher'),\n",
    "            ('Word Count', 'word_count', 'neither'),\n",
    "            ('CPU Usage (%)', 'cpu_usage_percent', 'lower'),\n",
    "            ('Memory (MB)', 'memory_mb', 'lower')\n",
    "        ]\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        for metric_name, metric_key, preference in metrics_to_compare:\n",
    "            val_3b = df_3b[metric_key].mean()\n",
    "            val_7b = df_7b[metric_key].mean()\n",
    "            \n",
    "            if preference == 'lower':\n",
    "                winner = '3B âœ“' if val_3b < val_7b else '7B âœ“'\n",
    "            elif preference == 'higher':\n",
    "                winner = '3B âœ“' if val_3b > val_7b else '7B âœ“'\n",
    "            else:\n",
    "                winner = '-'\n",
    "            \n",
    "            print(f\"{metric_name:<30} {val_3b:<15.3f} {val_7b:<15.3f} {winner:<10}\")\n",
    "            comparison_results[metric_key] = {'3b': val_3b, '7b': val_7b, 'winner': winner}\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Performance by complexity\n",
    "        print(\"3. PERFORMANCE BY COMPLEXITY:\")\n",
    "        print(\"-\"*80)\n",
    "        complexity_perf = in_scope_df.groupby(['model', 'complexity']).agg({\n",
    "            'quality_score': 'mean',\n",
    "            'latency_sec': 'mean'\n",
    "        }).round(3)\n",
    "        print(complexity_perf)\n",
    "        print()\n",
    "        \n",
    "        # Performance by category\n",
    "        print(\"4. PERFORMANCE BY QUESTION CATEGORY:\")\n",
    "        print(\"-\"*80)\n",
    "        category_perf = in_scope_df.groupby(['model', 'category']).agg({\n",
    "            'quality_score': 'mean',\n",
    "            'key_term_coverage': 'mean'\n",
    "        }).round(3)\n",
    "        print(category_perf)\n",
    "        print()\n",
    "        \n",
    "        # Hallucination test results\n",
    "        if len(out_scope_df) > 0:\n",
    "            print(\"5. HALLUCINATION TEST (Out-of-Scope Questions):\")\n",
    "            print(\"-\"*80)\n",
    "            hallucination_results = out_scope_df.groupby('model').agg({\n",
    "                'hallucination_score': ['mean', 'count']\n",
    "            }).round(3)\n",
    "            print(hallucination_results)\n",
    "            print(\"\\nNote: Score of 1.0 means model correctly refused to answer\")\n",
    "            print()\n",
    "        \n",
    "        return {\n",
    "            'summary': summary,\n",
    "            'comparison': comparison_results,\n",
    "            'complexity_performance': complexity_perf,\n",
    "            'category_performance': category_perf,\n",
    "            'hallucination_test': hallucination_results if len(out_scope_df) > 0 else None\n",
    "        }\n",
    "    \n",
    "    return {'summary': summary}\n",
    "\n",
    "\n",
    "def save_results(results_df: pd.DataFrame, analysis: Dict, output_dir: str = \"./experiment_results\"):\n",
    "    \"\"\"Save results to files\"\"\"\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    csv_path = f\"{output_dir}/model_comparison_{timestamp}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ“ Detailed results saved to: {csv_path}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = f\"{output_dir}/model_summary_{timestamp}.csv\"\n",
    "    analysis['summary'].to_csv(summary_path)\n",
    "    print(f\"âœ“ Summary saved to: {summary_path}\")\n",
    "    \n",
    "    # Save comparison as JSON\n",
    "    json_path = f\"{output_dir}/model_analysis_{timestamp}.json\"\n",
    "    analysis_serializable = {\n",
    "        'comparison': analysis.get('comparison', {}),\n",
    "        'timestamp': timestamp,\n",
    "        'models_tested': MODELS_TO_TEST\n",
    "    }\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(analysis_serializable, f, indent=2)\n",
    "    print(f\"âœ“ Analysis saved to: {json_path}\\n\")\n",
    "\n",
    "\n",
    "def plot_model_comparison(results_df: pd.DataFrame, output_dir: str = \"./experiment_results\"):\n",
    "    \"\"\"Generate comparison visualizations\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Separate in-scope questions\n",
    "        in_scope_df = results_df[results_df['category'] != 'Out-of-scope']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        fig.suptitle('Model Comparison: Qwen 3B vs 7B', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Latency comparison\n",
    "        in_scope_df.groupby('model')['latency_sec'].mean().plot(\n",
    "            kind='bar', ax=axes[0, 0], color=['skyblue', 'salmon']\n",
    "        )\n",
    "        axes[0, 0].set_title('Average Latency', fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Seconds')\n",
    "        axes[0, 0].set_xlabel('')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Plot 2: Quality score\n",
    "        in_scope_df.groupby('model')['quality_score'].mean().plot(\n",
    "            kind='bar', ax=axes[0, 1], color=['skyblue', 'salmon']\n",
    "        )\n",
    "        axes[0, 1].set_title('Answer Quality Score', fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].set_xlabel('')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "        axes[0, 1].set_ylim([0, 1])\n",
    "        \n",
    "        # Plot 3: Tokens per second\n",
    "        in_scope_df.groupby('model')['output_tokens_per_sec'].mean().plot(\n",
    "            kind='bar', ax=axes[0, 2], color=['skyblue', 'salmon']\n",
    "        )\n",
    "        axes[0, 2].set_title('Generation Speed', fontweight='bold')\n",
    "        axes[0, 2].set_ylabel('Tokens/sec')\n",
    "        axes[0, 2].set_xlabel('')\n",
    "        axes[0, 2].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Plot 4: Key term coverage\n",
    "        in_scope_df.groupby('model')['key_term_coverage'].mean().plot(\n",
    "            kind='bar', ax=axes[1, 0], color=['skyblue', 'salmon']\n",
    "        )\n",
    "        axes[1, 0].set_title('Key Term Coverage', fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Coverage')\n",
    "        axes[1, 0].set_xlabel('')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "        axes[1, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # Plot 5: Performance by complexity\n",
    "        complexity_data = in_scope_df.groupby(['model', 'complexity'])['quality_score'].mean().unstack()\n",
    "        complexity_data.plot(kind='bar', ax=axes[1, 1], width=0.7)\n",
    "        axes[1, 1].set_title('Quality by Complexity', fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Quality Score')\n",
    "        axes[1, 1].set_xlabel('')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "        axes[1, 1].legend(title='Complexity')\n",
    "        \n",
    "        # Plot 6: Resource usage\n",
    "        resource_data = in_scope_df.groupby('model')[['cpu_usage_percent', 'memory_mb']].mean()\n",
    "        resource_data['memory_gb'] = resource_data['memory_mb'] / 1024\n",
    "        resource_data[['cpu_usage_percent']].plot(kind='bar', ax=axes[1, 2], color=['skyblue', 'salmon'])\n",
    "        axes[1, 2].set_title('CPU Usage', fontweight='bold')\n",
    "        axes[1, 2].set_ylabel('CPU %')\n",
    "        axes[1, 2].set_xlabel('')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plot_path = f\"{output_dir}/model_comparison_plots_{timestamp}.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Plots saved to: {plot_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš  matplotlib/seaborn not installed. Skipping plots.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT 1: MODEL COMPARISON (Qwen 3B vs 7B)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Initialize ChromaDB\n",
    "    client, collection = initialize_chromadb(CHROMA_DB_PATH, COLLECTION_NAME)\n",
    "    \n",
    "    # Run experiment\n",
    "    results_df = run_model_comparison_experiment(\n",
    "        collection=collection,\n",
    "        gold_standard=gold_standard,\n",
    "        models=MODELS_TO_TEST,\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analysis = analyze_model_comparison(results_df)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results_df, analysis)\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_model_comparison(results_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"Sample Results:\")\n",
    "    print(results_df[['model', 'question_id', 'latency_sec', 'quality_score', 'output_tokens_per_sec']].head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
