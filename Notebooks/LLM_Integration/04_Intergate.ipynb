{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c4e8ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "Project root: c:\\Users\\patha\\Desktop\\Experiments\\Policy-as-Code-Engine-for-Academic-Compliance\n",
      "ChromaDB path: c:\\Users\\patha\\Desktop\\Experiments\\Policy-as-Code-Engine-for-Academic-Compliance\\Chroma_db_database\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Construct paths relative to notebook location\n",
    "# Notebook is in: Notebooks/LLM_integration/Integrate.ipynb\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "chroma_db_path = os.path.join(project_root, \"Chroma_db_database\")\n",
    "\n",
    "# Import libraries\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"ChromaDB path: {chroma_db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "705463f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection 'policy_docs' has 34 documents.\n",
      "✓ Collection is ready for querying.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect to ChromaDB\n",
    "client = chromadb.PersistentClient(path=chroma_db_path)\n",
    "\n",
    "# Get or create collection\n",
    "collection = client.get_or_create_collection(\"policy_docs\")\n",
    "\n",
    "# Check if collection has data\n",
    "count = collection.count()\n",
    "print(f\"\\nCollection 'policy_docs' has {count} documents.\")\n",
    "\n",
    "if count == 0:\n",
    "    print(\"\\n WARNING: Collection is empty!\")\n",
    "    print(\"Please run your embedding ingestion notebook first:\")\n",
    "    print(\"  - Notebooks/Data Ingestion/03_generate_embeddings_ollama.ipynb\")\n",
    "    print(\"  - Notebooks/Data Ingestion/04_ingest_to_chromadb.ipynb\")\n",
    "else:\n",
    "    print(\"✓ Collection is ready for querying.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0ca9540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG function defined successfully\n"
     ]
    }
   ],
   "source": [
    "def rag_answer(user_query, top_k=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Retrieves relevant policy documents and generates answer using LLM.\n",
    "    \n",
    "    Args:\n",
    "        user_query: The question to answer\n",
    "        top_k: Number of documents to retrieve\n",
    "        verbose: Print progress messages\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer with citations and sources\n",
    "    \"\"\"\n",
    "    if collection.count() == 0:\n",
    "        return \"Error: No documents in database. Please populate ChromaDB first.\"\n",
    "    \n",
    "    # 1. Embed the user question\n",
    "    if verbose:\n",
    "        print(f\"Embedding query: {user_query}\")\n",
    "    \n",
    "    query_embedding = ollama.embeddings(\n",
    "        model=\"nomic-embed-text:latest\",\n",
    "        prompt=user_query\n",
    "    )[\"embedding\"]\n",
    "\n",
    "    # 2. Fetch the most relevant docs\n",
    "    if verbose:\n",
    "        print(f\"Querying ChromaDB for top {top_k} documents...\")\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    context_docs = results[\"documents\"][0]\n",
    "    sources = results[\"metadatas\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    if not context_docs:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    # 3. Build the LLM context with sources\n",
    "    if verbose:\n",
    "        print(f\" Retrieved {len(context_docs)} documents.\\n\")\n",
    "    \n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[Source: {sources[i]['source']}]\\n{doc[:1000]}\"\n",
    "        for i, doc in enumerate(context_docs)\n",
    "    ])\n",
    "\n",
    "    # 4. Construct prompt for Qwen2.5\n",
    "    prompt = (\n",
    "    \"You are an expert in academic compliance regulations (UGC, AICTE, institutional bylaws). \"\n",
    "    \"Given the following regulatory sources and clauses, answer the question below accurately and comprehensively. \"\n",
    "    \"If the question asks for multiple items (objectives, guidelines, requirements, etc.), \"\n",
    "    \"list ALL items found in the context - do not summarize or condense. \"\n",
    "    \"Always cite the document source for each claim.\\n\\n\"\n",
    "    \"==== Context ====\\n\"\n",
    "    f\"{context}\\n\"\n",
    "    \"==== End Context ====\\n\\n\"\n",
    "    f\"Question: {user_query}\\n\\n\"\n",
    "    \"Answer (provide complete details with citations):\"\n",
    ")\n",
    "\n",
    "    # 5. Generate LLM answer\n",
    "    if verbose:\n",
    "        print(\"Generating answer \\n\")\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model=\"qwen2.5:7b\",\n",
    "        prompt=prompt\n",
    "    )[\"response\"]\n",
    "    \n",
    "    # 6. Return answer with source metadata\n",
    "    answer_with_sources = f\"{response}\\n\\n--- Retrieved Sources ---\\n\"\n",
    "    for i, src in enumerate(sources):\n",
    "        answer_with_sources += f\"{i+1}. {src['source']} (distance: {distances[i]:.4f})\\n\"\n",
    "    \n",
    "    return answer_with_sources\n",
    "\n",
    "print(\"✓ RAG function defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Interactive mode: Keep asking questions until 'quit'\n",
    "# ============================================================\n",
    "print(\"Type your question and press Enter. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Your question: \").strip()\n",
    "    \n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    \n",
    "    if not user_input:\n",
    "        print(\"Please enter a question.\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(\"\\nProcessing...\\n\")\n",
    "    answer = rag_answer(user_input, top_k=3, verbose=True)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Answer:\")\n",
    "    print(\"=\"*60)\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
